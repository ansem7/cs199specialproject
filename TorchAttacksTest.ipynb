{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansem7/cs199specialproject/blob/main/TorchAttacksTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7DO3_acNvW9"
      },
      "source": [
        "## **1. Install torchattacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DNUCI_rUNvJN",
        "outputId": "3b90de77-b613-44eb-a3c2-f8b90b15e7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchattacks in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.18.0+cu121)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.66.4)\n",
            "Requirement already satisfied: requests~=2.25.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.25.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->torchattacks) (12.5.40)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->torchattacks) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\n",
            "--2024-06-08 12:34:38--  https://raw.githubusercontent.com/raghakot/keras-vis/master/resources/imagenet_class_index.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35363 (35K) [text/plain]\n",
            "Saving to: ‘imagenet_class_index.json’\n",
            "\n",
            "imagenet_class_inde 100%[===================>]  34.53K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-06-08 12:34:39 (3.30 MB/s) - ‘imagenet_class_index.json’ saved [35363/35363]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install torchattacks\n",
        "# ! wget -O apple.jpg https://images.everydayhealth.com/images/diet-nutrition/apples-101-about-1440x810.jpg # Replace this(the second) url for another image. (It's currently an apple)\n",
        "! wget -O imagenet_class_index.json https://github.com/ansem7/cs199specialproject/blob/e22b91ccf47246a081323abbf952b0d030b6faf6/imagenet_class_index.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4DTPvdAMfFo"
      },
      "source": [
        "## **2. Import the necessary libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "collapsed": true,
        "id": "NEZM9AX6L4Hc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QoS636UN7k_"
      },
      "source": [
        "## **3. Load the pretrained ResNet50 model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "25ksUuQsMm-n"
      },
      "outputs": [],
      "source": [
        "model = resnet50(pretrained=True).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABV5dSvaOBEE"
      },
      "source": [
        "## **4. Define the transformation for your image:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "TqX_RRGoODN0"
      },
      "outputs": [],
      "source": [
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),  # Convert image to RGB if it's not\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxs3SZFuOG0B"
      },
      "source": [
        "## **5. Load your images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKxxM8SgOIdS",
        "outputId": "821adb0d-6b17-43a6-e993-bf3f3c1f92e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "dir_path = \"/content/test\"\n",
        "\n",
        "# Create the directory\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your images in Google Drive\n",
        "image_path = '/content/drive/My Drive/partial_images2'\n",
        "\n",
        "# image_path = '/content/test'\n",
        "\n",
        "# Load images from the directory\n",
        "image_folder = datasets.ImageFolder(image_path, transform=transform)\n",
        "\n",
        "# Create a data loader\n",
        "image_loader = torch.utils.data.DataLoader(image_folder, batch_size=32)\n",
        "\n",
        "# image = Image.open('apple.jpg')\n",
        "# image = transform(image).unsqueeze(0)  <--- these two are used for a single image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmVXXFiQOTi2"
      },
      "source": [
        "## **6. Create an instance of the chosen attack**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "tJe5kH2ZOVZF"
      },
      "outputs": [],
      "source": [
        "from torchattacks import FGSM, PGD, CW, PGDL2, NIFGSM, Pixle, SINIFGSM, VMIFGSM, VNIFGSM, LGV\n",
        "\n",
        "# Define the attack\n",
        "# FGSM\n",
        "#attack = FGSM(model, eps=0.3)\n",
        "\n",
        "# PGD\n",
        "#attack = PGD(model, eps=8/255, alpha=2/255, steps=2)\n",
        "\n",
        "# CW\n",
        "#attack = CW(model, c=1, kappa=0, steps=2, lr=0.01)\n",
        "\n",
        "# PGDL2\n",
        "#attack = PGDL2(model, eps=0.3, alpha=0.01, steps=2)\n",
        "\n",
        "# NIFGSM\n",
        "#attack = NIFGSM(model, eps=0.2)\n",
        "\n",
        "# Pixle\n",
        "attack = Pixle(model, x_dimensions=(2, 10), y_dimensions=(2, 10), pixel_mapping='random', restarts=20, max_iterations=10, update_each_iteration=False)\n",
        "\n",
        "# SINIFGSM\n",
        "#attack = SINIFGSM(model, eps=8/255, alpha=2/255, steps=2, decay=1.0, m=5)\n",
        "\n",
        "# VMIFGSM\n",
        "#attack = VMIFGSM(model, eps=8/255, alpha=2/255, steps=2, decay=1.0)\n",
        "\n",
        "# VNIFGSM\n",
        "#attack = VNIFGSM(model, eps=8/255, alpha=2/255, steps=2, decay=1.0, N=5, beta=3/2)\n",
        "\n",
        "# Lists to store true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMTE-mPSOYIn"
      },
      "source": [
        "## **7. Apply the attack to your image:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "0BQo_KwsOZuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2817b0-028e-481a-e41c-bfbe7ab625b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attack took 48.38 sec to complete.\n"
          ]
        }
      ],
      "source": [
        "# adv_image = attack(image, torch.tensor([322]))  # 948 is the ImageNet class index for 'apple'\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load the ImageNet class index file\n",
        "with open('imagenet_class_index.json') as f:\n",
        "    class_idx = json.load(f)\n",
        "\n",
        "# Now you can use this dictionary to get the label name from an index\n",
        "#def get_label_name(index):\n",
        "\n",
        "#    return class_idx[index]\n",
        "\n",
        "def get_label_name(index):\n",
        "      return class_idx[f\"{index}\"]\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "total_attack_time = 0  # Initialize a variable to store the total attack time\n",
        "\n",
        "# Iterate over the images\n",
        "for i, (images, labels) in enumerate(image_loader):\n",
        "\n",
        "    #print(f\"{i}: {labels}\")\n",
        "\n",
        "    # Start the timer before the attack\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Apply the attack to the images\n",
        "    adv_images = attack(images, labels)\n",
        "\n",
        "    # End the timer after the attack and add the elapsed time to the total\n",
        "    total_attack_time += time.time() - start_time\n",
        "\n",
        "    # Pass the adversarial images through the model\n",
        "    outputs = model(adv_images)\n",
        "\n",
        "    # Get the predicted labels\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # Convert the predicted labels to their classification names\n",
        "    predicted_labels_names = [get_label_name(label)[1] for label in preds.tolist()]\n",
        "\n",
        "    # Convert the true labels to their classification names\n",
        "    true_labels_names = [image_loader.dataset.classes[label] for label in labels.tolist()]\n",
        "\n",
        "    predicted_labels.extend(predicted_labels_names)\n",
        "    true_labels.extend(true_labels_names)\n",
        "\n",
        "    #print(f\"Predicted labels names: {predicted_labels_names}\")\n",
        "    #print(f\"Actual labels: {true_labels_names}\")\n",
        "\n",
        "    # Save the adversarial images\n",
        "    #for j, adv_image in enumerate(adv_images):\n",
        "        # Convert the tensor to an image\n",
        "    #    adv_image = transforms.ToPILImage()(adv_image)\n",
        "\n",
        "        # Get the predicted label for the current image\n",
        "    #    pred_label_name = get_label_name(preds[j].item())[1]\n",
        "\n",
        "        # Save the image with the filename set to its classification along with its indices i and j\n",
        "    #    adv_image.save(f'/content/test/adv_image_{pred_label_name}_{i}_{j}.png')\n",
        "\n",
        "print(f\"The attack took {round(total_attack_time, 2)} sec to complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculate the robust accuracy and the confusion matrix** of the ResNet50 model based on its performance after feeding the resulting images of the adversarial attack."
      ],
      "metadata": {
        "id": "e94nDydFoHzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
        "\n",
        "print(f\"Predicted labels: {predicted_labels}\")\n",
        "print(f\"Actual labels: {true_labels}\")\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f'Accuracy: {accuracy * 100}%')\n",
        "\n",
        "# Calculate the precision score\n",
        "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "print(f'Precision: {precision * 100}%')\n",
        "\n",
        "# Calculate the recall score\n",
        "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "print(f'Recall: {recall * 100}%')\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "print(f'F1 Score: {f1 * 100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJJwzo87oTlf",
        "outputId": "58b5aaa5-d528-4326-a845-c02b285f748b"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: ['Granny_Smith', 'pomegranate', 'Granny_Smith', 'hip', 'strawberry', 'bell_pepper', 'candle', 'Granny_Smith', 'Granny_Smith', 'pomegranate', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'buckeye', 'hip', 'hip', 'head_cabbage', 'buckeye', 'hip', 'fig', 'hip', 'pomegranate', 'plunger', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper']\n",
            "Actual labels: ['Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'Granny_Smith', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'beetroot', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper', 'bell_pepper']\n",
            "Accuracy: 60.0%\n",
            "Precision: 24.242424242424242%\n",
            "Recall: 20.0%\n",
            "F1 Score: 21.03174603174603%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The code below will be used to test the resulting images of the attacks.\n",
        "This is to manually check if the resulting classifications of each image matches the correct computation of the robust accuracy, f1-scores and the confusion matrix."
      ],
      "metadata": {
        "id": "rboK17QrhV5m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "0Eshjfx9QbiY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGzIcrdrQfej"
      },
      "source": [
        "**11. Load Pre-Trained Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DJWDlAiBQhLR",
        "outputId": "b4aa9f0a-b8c0-4d35-bbd0-b23545017645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ-_POR0Qirn"
      },
      "source": [
        "**12. Preprocess the Image:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "WcqsCSD0QmHw"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zz1gs2gQpKB"
      },
      "source": [
        "**13. Load and Preprocess Your Image:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "26vmNACRQr9T"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open(\"/content/test/adv_image_banana_0_10.png\").convert(\"RGB\")\n",
        "img_t = preprocess(img)\n",
        "batch_t = torch.unsqueeze(img_t, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "215pQYT9QyM6"
      },
      "source": [
        "**14. Download the ImageNet Class Index File:** You can download the ImageNet class index file (a JSON file that maps class indices to labels) from this link. Save it in your working directory.\n",
        "\n",
        "**15. Load the Class Index File:** You can load the class index file using the json module in Python. The loaded object is a dictionary that maps indices to class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "syjgx497Q2oQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Upload the json file to google colab first before running this.\n",
        "with open(\"imagenet_class_index.json\") as f:\n",
        "    class_idx = json.load(f)\n",
        "\n",
        "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1VjfBZbQ92H"
      },
      "source": [
        "In this code, idx2label is a list of class labels in the correct order.\n",
        "\n",
        "**16. Get the class name of the predicted class:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFdGBZ2kRFax",
        "outputId": "8cbbb6c1-d5d9-4aef-f894-56cdc96a0e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model predicts that the image belongs to: banana.\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    output = model(batch_t)\n",
        "_, predicted_idx = torch.max(output, 1)\n",
        "\n",
        "# Get the name of the predicted class\n",
        "predicted_class = idx2label[predicted_idx.item()]\n",
        "\n",
        "print(f\"The model predicts that the image belongs to: {predicted_class}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anT3H25_BmgV"
      },
      "source": [
        "**References**\n",
        "\n",
        "https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}